# -*- coding: utf-8 -*-
"""VAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0iuwAlE6OZXUJZoHzQvpxaaUBJ8_tKr
"""

!pip install torch numpy scikit-learn matplotlib

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# ======================
# Reproducibility
# ======================
torch.manual_seed(42)
np.random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ======================
# 1. Dataset Generation
# ======================
def generate_data(n_samples=12000, n_features=20, anomaly_ratio=0.05):
    X_normal, _ = make_blobs(
        n_samples=int(n_samples * (1 - anomaly_ratio)),
        centers=5,
        n_features=n_features,
        cluster_std=1.2,
        random_state=42
    )

    X_anomaly = np.random.uniform(
        low=-10, high=10,
        size=(int(n_samples * anomaly_ratio), n_features)
    )

    X = np.vstack([X_normal, X_anomaly])
    y = np.hstack([
        np.zeros(len(X_normal)),
        np.ones(len(X_anomaly))
    ])

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    return train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

X_train, X_test, y_train, y_test = generate_data()

X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
X_test = torch.tensor(X_test, dtype=torch.float32).to(device)

# ======================
# 2. VAE Architecture
# ======================
class Encoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        self.mu = nn.Linear(32, latent_dim)
        self.logvar = nn.Linear(32, latent_dim)

    def forward(self, x):
        h = self.fc(x)
        return self.mu(h), self.logvar(h)


class Decoder(nn.Module):
    def __init__(self, latent_dim, output_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )

    def forward(self, z):
        return self.fc(z)


class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = Encoder(input_dim, latent_dim)
        self.decoder = Decoder(latent_dim, input_dim)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decoder(z)
        return x_recon, mu, logvar

# ======================
# 3. Loss & Training
# ======================
def vae_loss(x, x_recon, mu, logvar, beta):
    recon_loss = nn.MSELoss(reduction='mean')(x_recon, x)
    kl_div = -0.5 * torch.mean(
        1 + logvar - mu.pow(2) - logvar.exp()
    )
    return recon_loss + beta * kl_div, recon_loss, kl_div

latent_dim = 8
model = VAE(X_train.shape[1], latent_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

epochs = 50
batch_size = 128
beta_max = 1.0

def train():
    model.train()
    for epoch in range(epochs):
        perm = torch.randperm(X_train.size(0))
        beta = min(beta_max, epoch / 20)

        for i in range(0, X_train.size(0), batch_size):
            idx = perm[i:i+batch_size]
            batch = X_train[idx]

            optimizer.zero_grad()
            x_recon, mu, logvar = model(batch)
            loss, _, _ = vae_loss(batch, x_recon, mu, logvar, beta)
            loss.backward()
            optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch} | Beta {beta:.2f} | Loss {loss.item():.4f}")

train()

# ======================
# 4. Anomaly Scoring
# ======================
def anomaly_score(model, x):
    model.eval()
    with torch.no_grad():
        x_recon, mu, logvar = model(x)
        recon_error = torch.mean((x - x_recon)**2, dim=1)
        kl = -0.5 * torch.sum(
            1 + logvar - mu.pow(2) - logvar.exp(), dim=1
        )
    return (recon_error + kl).cpu().numpy()

scores = anomaly_score(model, X_test)

# Threshold selection using F1
best_f1 = 0
best_thresh = None
for t in np.percentile(scores, np.arange(80, 99)):
    preds = (scores > t).astype(int)
    _, _, f1, _ = precision_recall_fscore_support(y_test, preds, average='binary')
    if f1 > best_f1:
        best_f1 = f1
        best_thresh = t

y_pred = (scores > best_thresh).astype(int)

precision, recall, f1, _ = precision_recall_fscore_support(
    y_test, y_pred, average='binary'
)
auc = roc_auc_score(y_test, scores)

print("\nVAE RESULTS")
print(f"Precision: {precision:.3f}")
print(f"Recall:    {recall:.3f}")
print(f"F1 Score:  {f1:.3f}")
print(f"ROC AUC:   {auc:.3f}")

# ======================
# 5. Baseline: Isolation Forest
# ======================
iso = IsolationForest(contamination=0.05, random_state=42)
iso.fit(X_train.cpu().numpy())
iso_scores = -iso.score_samples(X_test.cpu().numpy())
iso_auc = roc_auc_score(y_test, iso_scores)

print("\nISOLATION FOREST ROC AUC:", iso_auc)