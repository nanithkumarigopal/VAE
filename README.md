Variational Autoencoder (VAE) for Anomaly Detection

This project implements a Variational Autoencoder (VAE) from scratch using PyTorch for unsupervised anomaly detection on a high-dimensional dataset. The model leverages both reconstruction error and latent space divergence (KL divergence) to identify anomalous samples and compares performance against a classical baseline (Isolation Forest).

ğŸ“Œ Project Overview

Anomaly detection is critical in domains such as:

Network intrusion detection

Fraud detection

Sensor fault diagnosis

This project demonstrates how probabilistic latent variable models, specifically VAEs, can effectively learn the underlying distribution of normal data and flag deviations as anomalies.

ğŸš€ Key Features

Custom VAE implementation (Encoder, Decoder, Reparameterization Trick)

Î²-VAE with KL Annealing to stabilize training

High-dimensional synthetic dataset with injected anomalies

Robust anomaly scoring mechanism

Quantitative evaluation using Precision, Recall, F1-Score, ROC AUC

Performance comparison with Isolation Forest

ğŸ—‚ Project Structure
.
â”œâ”€â”€ vae.py        # Main Python implementation
â””â”€â”€ README.md     # Project documentation

ğŸ“Š Dataset Description

The dataset is programmatically generated using sklearn.make_blobs:

Total samples: 10,000

Features: 20

Normal data: Clustered Gaussian blobs

Anomalies: Uniformly distributed outliers

Anomaly ratio: 5%

All features are standardized using StandardScaler.

ğŸ— Model Architecture
Encoder

Input â†’ 128 â†’ 64

Outputs:

Mean vector (Î¼)

Log-variance vector (log ÏƒÂ²)

Latent Space

Latent dimension: 8

Sampling via reparameterization trick

Decoder

Latent â†’ 64 â†’ 128 â†’ Output

ğŸ§® Loss Function

The VAE optimizes the following objective:

ğ¿
=
Reconstruction Loss
+
ğ›½
â‹…
KL Divergence
L=Reconstruction Loss+Î²â‹…KL Divergence

Reconstruction Loss: Mean Squared Error (MSE)

KL Divergence: Regularizes latent space to follow a standard normal distribution

Î² (Beta): Controls strength of latent regularization

ğŸ”§ Optimization Strategy

Optimizer: Adam (lr = 1e-3)

Batch size: 128

Epochs: 50

KL Annealing:
Gradually increases Î² from 0 â†’ 1 over the first 20 epochs to prevent posterior collapse.

ğŸš¨ Anomaly Detection Method

An anomaly score is computed for each sample as:

Anomaly Score
=
Reconstruction Error
+
KL Divergence
Anomaly Score=Reconstruction Error+KL Divergence

Samples above the 95th percentile of anomaly scores are classified as anomalies.

ğŸ“ˆ Evaluation Metrics

Precision

Recall

F1-Score

ROC AUC

Evaluation is performed on a held-out test set with known anomalies.

ğŸ“Š Baseline Comparison

A scikit-learn Isolation Forest model is used as a baseline for comparison.

Model	Strengths	Weaknesses
VAE (Î²-VAE)	Learns data distribution, probabilistic	Requires tuning
Isolation Forest	Fast, simple	Less effective in high dimensions

The VAE consistently achieves higher F1-Score and ROC AUC due to its ability to model complex data distributions.

â–¶ï¸ How to Run
Install Dependencies
pip install numpy torch scikit-learn matplotlib

Run the Project
python vae.py


The script will:

Generate the dataset

Train the VAE

Detect anomalies

Evaluate performance

Compare with Isolation Forest

 Key Learning Outcomes

Understanding of Variational Autoencoders

Practical implementation of KL divergence & reparameterization

Importance of latent space regularization

Application of deep generative models for anomaly detection

Performance comparison with classical ML methods

ğŸ“Œ Future Improvements

Apply to real datasets (e.g., KDD Cup 1999)

Visualize latent space using t-SNE or PCA

Adaptive threshold selection via ROC curve

Convolutional VAE for structured data

ğŸ“„ License

This project is intended for academic and educational use.



